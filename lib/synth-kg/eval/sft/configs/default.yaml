sft_config:
  _target_: trl.SFTConfig
  per_device_train_batch_size: 5
  logging_steps: 10
  remove_unused_columns: true
  learning_rate: 5e-5
  warmup_ratio: 0
  group_by_length: true
  max_seq_length: 512
  lr_scheduler_type: "cosine_with_min_lr"
  lr_scheduler_kwargs: { "min_lr": 1e-6 }
  num_train_epochs: 10
  save_safetensors: false
  save_strategy: "epoch"
  report_to: "wandb"
  output_dir: "/lora/sft"

model_config:
  _target_: trl.ModelConfig
  use_peft: true
  torch_dtype: "bfloat16"
  lora_r: 32
  lora_alpha: 64
  lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  model_name_or_path: meta-llama/llama-2-7b-hf

dataset: ???
tags: ???
