dpo_config:
  _target_: trl.DPOConfig
  beta: 0.1
  bf16: true
  eval_strategy: "no"
  gradient_checkpointing: true
  group_by_length: true
  learning_rate: 5e-6
  logging_steps: 10
  weight_decay: 0.01
  max_length: 512
  num_train_epochs: 5
  output_dir: "models/dpo/"
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 1
  remove_unused_columns: false
  save_only_model: true
  save_safetensors: false
  save_strategy: "no"
  model_adapter_name: "default"
  ref_adapter_name: "reference"
  seed: 0
  warmup_ratio: 0

peft_config:
  _target_: peft.LoraConfig
  r: 32
  lora_alpha: 64
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

model_config:
  _target_: trl.ModelConfig
  torch_dtype: "bfloat16"
  model_name_or_path: meta-llama/llama-2-7b-hf

dataset: ???
tags: ???
iteration: 1
peft_adapters_paths: ???
