import json

import requests as rq

from .base import Model


class LlamaModel(Model):
    """Llama model that can be run with prompts.

    Details on ways to access a quantized llama deployed on GCP can be found here:
    https://github.com/arkhn/tiny-llama

    Args:
        url: URL on which the model is deployed.
        min_gen_len: Minimum length of the generated text.
        max_gen_len: Maximum length of the generated text.
        temperature: The value used to module the next token probabilities.
        top_p: The cumulative probability for top-p filtering.
    """

    def __init__(
        self,
        url: str,
        min_gen_len: int = 0,
        max_gen_len: int = 128,
        temperature: float = 0,
        top_p: float = 0.95,
    ) -> None:
        self.url = url
        self.min_gen_len = min_gen_len
        self.max_gen_len = max_gen_len
        self.temperature = temperature
        self.top_p = top_p

    def run(self, prompt: str) -> str:
        """
        Run the model on `prompt`.

        Args:
            prompt: Prompt to give the model.

        Returns:
            The result generated by the model.
        """
        response: rq.Response = rq.post(
            url=self.url,
            json={
                "prompters": [prompt],
                "min_gen_len": self.min_gen_len,
                "max_gen_len": self.max_gen_len,
                "temperature": self.temperature,
                "top_p": self.top_p,
            },
        )
        response_dict = json.loads(response.text)
        return response_dict["responses"][0]
