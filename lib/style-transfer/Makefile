help:  ## Show help
	@grep -E '^[.a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-30s\033[0m %s\n", $$1, $$2}'

clean: ## Clean autogenerated files
	rm -rf dist
	find . -type f -name "*.DS_Store" -ls -delete
	find . | grep -E "(__pycache__|\.pyc|\.pyo)" | xargs rm -rf
	find . | grep -E ".pytest_cache" | xargs rm -rf
	find . | grep -E ".ipynb_checkpoints" | xargs rm -rf
	rm -f .coverage

create-mimic-dataset: ## don't forget to install and initialize quickumls and set env called QUICKUMLS_PATH
	cd hf_datasets/mimic_iii && \
	python3 -m _preprocessing.py && \
	python3 -m _keywords_extraction.py && \
	./split.sh

create-pmc-datasets: ## don't forget to install and initialize quickumls and set env called QUICKUMLS_PATH
	cd hf_datasets/pmc_patients && \
	python3 -m _preprocessing.py && \
	python3 -m _keywords_extraction.py && \
	./split.sh

create-yelp-datasets: ## this create the dataset for the yelp dataset
	cd hf_datasets/yelp && \
	python3 -m _preprocessing.py && \
	python3 -m _keywords_extraction.py && \
	./split.sh

create-datasets-cards: ## create metadata and test hf datasets
	cd hf_datasets/yelp && \
	datasets-cli test yelp_style_transfer.py --save_info --all_configs && \
	cd ../mimic_iii && \
	datasets-cli test mimic_style_transfer.py --save_info --all_configs && \
	cd ../pmc_patients && \
	datasets-cli test pmc_style_transfer.py --save_info --all_configs

start: ## start the training
	accelerate launch --num_cpu_threads_per_process=16 --config_file=accelerate-config.yaml dpo.py
