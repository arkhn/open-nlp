# @package _global_

# specify here default configuration
# order of defaults determines the order in which configs override each other

ppo_config:
  _target_: trl.PPOConfig
  model_name: "mistralai/Mistral-7B-Instruct-v0.1"
  learning_rate: 2e-5
  log_with: "wandb"
  ppo_epochs: 100
  tracker_project_name: "style-transfer-ppo"
  init_kl_coef: 2
  mini_batch_size: 4
  batch_size: 16
  gradient_accumulation_steps: 4
  optimize_cuda_cache: True
  use_score_scaling: True
  use_score_norm: True
  score_clip: 1
  remove_unused_columns: False

max_sampler_length: 256

dataset: "bio-datasets/mimic_style_transfer"

lora:
  _target_: peft.LoraConfig
  task_type: CAUSAL_LM
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  bias: none
  target_modules: ["q_proj", "v_proj"]

bnb:
  _target_: transformers.BitsAndBytesConfig
  load_in_4bit: true
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: True
  bnb_4bit_compute_dtype: bfloat16

model:
  num_shared_layers: 20
  use_flash_attention_2: True
  save_path: "models/ppo"

generation_config:
  _target_: transformers.GenerationConfig
  min_length: -1
  top_p: 1.0
  num_beams: 1
  repetition_penalty: 1.3
  do_sample: true

seed: 0
